{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Jupyter Notebook UI to analyze baseline data from tap-habituation experiments!\n",
    "\n",
    "### Beginner Essentials:\n",
    "1. Shift-Enter to run each cell. After you run, you should see an output \"done step #\". If not, an error has occured\n",
    "2. When inputting your own code/revising the code, make sure you close all your quotation marks '' and brackets (), [], {}.\n",
    "3. Don't leave any commas (,) hanging! (make sure an object always follows a comma. If there is nothing after a comma, remove the comma!\n",
    "4. Learning to code? Each line of code is annotated to help you understand how this code works!\n",
    "\n",
    "**Run all cells/steps sequentially, even the ones that do not need input**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Analysis of the Jupyter Notebook\n",
    "\n",
    "| Step | Purpose | Key Actions |\n",
    "|------|---------|-------------|\n",
    "| **1. Import Packages** | Load required Python libraries for data analysis | Imports `pandas`, `numpy`, `matplotlib`, etc. | \n",
    "| **2. Pick Filepath** | Select the folder containing experimental data files (.dat or .trv) | Input required: Uses `FileChooser` widget to select directory | \n",
    "| **3. User-Defined Variables** | Set experiment parameters | Defines: `bin`  | \n",
    "| **4. Construct Filelist** | Find all files in selected folder | Sets working directory and scans `folder_path` using; Displays no. of `.trv` files found in the folder |\n",
    "| **5. Process Data Function** | Define functions to load, clean, and analyze raw data | - `ProcessData()`: Loads files, calculates metrics (reversal probability, speed) |\n",
    "| **6.1 Process Data** | Apply processing to all strains| - Checks `filelist` for unique strain names (e.g., \"N2\") <br>- Runs `ProcessData()` for each strain | \n",
    "| **7. Grouping & Naming** | Combine data from all strains | - Concatenates DataFrames<br>- Assigns dataset names (e.g., \"N2\") | \n",
    "| **Output CSV** | Save processed data | Exports `Baseline_data` to CSV |\n",
    "\n",
    "### Key Notes:\n",
    "- User Input Required: Steps 2 (file selection), 3 (parameters), 6.1 (strain verification)\n",
    "- Output: Final CSV contains all analyzed tap response data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Packages Required (No input required, just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #<- package used to import and organize data\n",
    "import numpy as np #<- package used to import and organize data\n",
    "import seaborn as sns #<- package used to plot graphs\n",
    "from matplotlib import pyplot as plt #<- package used to plot graphs\n",
    "import os #<- package used to work with system filepaths\n",
    "from ipywidgets import widgets #<- widget tool to generate button\n",
    "from IPython.display import display #<- displays button\n",
    "from ipyfilechooser import FileChooser\n",
    "# from tkinter import Tk, filedialog #<- Tkinter is a GUI package\n",
    "from tqdm.notebook import tqdm\n",
    "# import dask.dataframe as dd\n",
    "# import pingouin as pg\n",
    "pd.set_option('display.max_columns', 50)\n",
    "print(\"done step 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pick filepath (just run and click button from output)\n",
    "\n",
    "Run the following cell and click the button 'Select Folder' to pick a filepath.\n",
    "\n",
    "**Important: Later on, this script uses the total file path for each file to import and group data. That means if your folder has whatever your strain is named, the script will not work.**\n",
    "\n",
    "(ex. if your folder has \"N2\" in it this script sees all files inside this folder as having the \"N2\" search key)\n",
    "\n",
    "**An easy fix is to just rename your folder to something else (make your strains lower-case, or just have the date)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_directory = '/Users'\n",
    "chooser = FileChooser(starting_directory)\n",
    "display(chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chooser.selected_path)\n",
    "folder_path=chooser.selected_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screens = ['PD_Screen', 'ASD_Screen', 'G-Proteins_Screen', 'Glia_Genes_Screen', 'Neuron_Genes_Screen', 'Miscellaneous']\n",
    "\n",
    "screen_chooser = widgets.Select(options=screens, value=screens[0], description='Screen:')\n",
    "display(screen_chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Screen=screen_chooser.value\n",
    "print(Screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. User Defined Variables (Add input here)\n",
    "\n",
    "Here, we add some constants to help you blaze through this code.\n",
    "\n",
    "3.1: Setting time bins\n",
    "\n",
    "\n",
    "3.2: Setting view range for your graph\n",
    "- Top, bottom = y axis view range\n",
    "- left, right = x axis view range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 1s Bins\n",
    "bins = np.linspace(0,1200,1201) # np.linspace(start, end, steps in between)\n",
    "print(bins)\n",
    "\n",
    "\n",
    "print(\"done step 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Construct filelist from folder path (No input required, just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(folder_path) # setting your working directory so that your images will be saved here\n",
    "\n",
    "filelist = list() # empty list\n",
    "for root, dirs, files in os.walk(folder_path): # this for loop goes through your folder \n",
    "    for name in files:\n",
    "        if name.endswith('.dat'): # and takes out all files with a .dat (file that contains your data)\n",
    "            if \"_\" in name.split(\".\")[-2]:\n",
    "                filepath = os.path.join(root, name) # Notes down the file path of each data file\n",
    "                filelist.append(filepath) # saves it into the list\n",
    "\n",
    "if not filelist:\n",
    "    raise FileNotFoundError(\"No .dat files found in the selected folder!\")\n",
    "else:\n",
    "    print(f\"Number of .dat files to process: {len(filelist)}\")\n",
    "    # print(f\"Example of first and last file saved: {filelist[0]}, {filelist[-1]}\") \n",
    "\n",
    "print('done step 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Process Data Function (No input required, just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessData(strain, experiment_counter): \n",
    "    \"\"\"\n",
    "    Filters and processes .dat files matching the given strain.\n",
    "\n",
    "    Parameters: \n",
    "        strain (str): keyword to match in the files\n",
    "\n",
    "    Returns:\n",
    "        dict: N (Plate number) and Dataframe with required columns \n",
    "              (\"time\", \"dura\", \"dist\", \"prob\", \"speed\", \"plate\", \"Date\",\n",
    "              \"Plate_id\", \"Screen\")\n",
    "\n",
    "    \"\"\"\n",
    "    strain_filelist = [x for x in filelist if strain in x] # Goes through the list and filters for keyword\n",
    "    Strain_N = len(strain_filelist) # Finds the number of plates per strain\n",
    "    if Strain_N == 0:\n",
    "        raise AssertionError ('{} is not a good identifier'.format(strain))\n",
    "    else:\n",
    "        pass\n",
    "        print(f'Strain {strain}')\n",
    "        print(f'Number of plates: {Strain_N}') \n",
    "        \n",
    "        # visiting files in this strain\n",
    "        strain_filelist = [file for file in filelist if strain in file]\n",
    "        df_list=[]\n",
    "        for i, file in enumerate(strain_filelist):\n",
    "            if file.split('/')[-1].startswith('._'):\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    print(f\"File: {file}\")\n",
    "                    df= pd.read_csv(file, sep=' ', header = None, encoding_errors='ignore')\n",
    "                    df['Plate_id'] = file.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "                    df['Date'] = file.split('/')[-2].split('_')[0]\n",
    "                    df['Screen'] = file.split('/')[-4]\n",
    "                    df['Experiment'] = experiment_counter\n",
    "                    experiment_counter = 1+experiment_counter\n",
    "                    df_list.append(df)\n",
    "                except:\n",
    "                    print(f\"error in file {file}\")\n",
    "                    pass\n",
    "        DF_Total = pd.concat(df_list, ignore_index = True)\n",
    "        DF_Total = DF_Total.rename( \n",
    "                    {0:'Time',\n",
    "                    1:'n',\n",
    "                    2:'Number',\n",
    "                    3:'Instantaneous Speed',\n",
    "                    4:'Interval Speed',\n",
    "                    5:'Bias',\n",
    "                    6:'Tap',\n",
    "                    7:'Puff',\n",
    "                    8:'x',\n",
    "                    9:'y',\n",
    "                    10:'Morphwidth',\n",
    "                    11:'Midline',\n",
    "                    12:'Area',\n",
    "                    13:'Angular Speed',\n",
    "                    14:'Aspect Ratio',\n",
    "                    15:'Kink',\n",
    "                    16:'Curve',\n",
    "                    17:'Crab',\n",
    "                    18:'Pathlength'}, axis=1)\n",
    "        \n",
    "        # check function here for NaN Columns\n",
    "        DF_Total['plate'] = 0\n",
    "\n",
    "        print(\"---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    return{\n",
    "            'N': Strain_N,\n",
    "            'Confirm':DF_Total,\n",
    "            'experiment_counter': experiment_counter\n",
    "            # 'Final': DF_Final\n",
    "    }\n",
    "\n",
    "\n",
    "print('done step 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Process Data\n",
    "\n",
    "Create a dictionary `StrainNames` that contains all the genotype/strain names from each file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genotype=[]\n",
    "for f in filelist:\n",
    "    genotype.append(f.split('/')[-3])\n",
    "\n",
    "genotypes=np.unique(genotype).tolist()\n",
    "\n",
    "if Screen ==\"Neuron_Genes_Screen\":\n",
    "    genotypes.insert(0, genotypes.pop(genotypes.index(\"N2_XJ1\")))\n",
    "    genotypes.insert(0, genotypes.pop(genotypes.index(\"N2_N2\")))\n",
    "else:\n",
    "    genotypes.insert(0, genotypes.pop(genotypes.index(\"N2\")))\n",
    "\n",
    "nstrains = list(range(1, len(genotypes) + 1))\n",
    "StrainNames = {nstrains[i]: genotypes[i] for i in range(len(nstrains))}\n",
    "\n",
    "print(f\"Number of genotypes/strains in the experiment: {len(genotypes)}\")\n",
    "\n",
    "# Display the first 5 Strain names in the experiment\n",
    "for k in list(StrainNames)[:5]:\n",
    "    print(f\"{k}: {StrainNames[k]}\")\n",
    "\n",
    "\n",
    "print(\"done step 6.1\")\n",
    "\n",
    "# <---------------- Test element to use for dictionary buidling -------------------\n",
    "# s = '/Users/Joseph/Desktop/OnFoodOffFoodTest/N2_OnFood/20220401_163048/N2_10x1_n96h20C_360sA0401_ka.00065.dat'\n",
    "# slist=s.split('/')[5]\n",
    "# print(slist)\n",
    "# print(list(range(1,5+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Process Data (just run this cell)\n",
    "\n",
    "Pass each strain through `ProcessData()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLists = [0] # generates empty list at index 0 because we want indexing to start at 1 \n",
    "                # when I say #1, I want the first point, not the second point\n",
    "\n",
    "experiment_counter = 1\n",
    "\n",
    "# the loop below goes through the dictionary in step 6.1 and processes data\n",
    "# and appends all data into a list of dataframes\n",
    "for s in tqdm(StrainNames.values()): \n",
    "    if not s == '':\n",
    "        result = ProcessData(s, experiment_counter)\n",
    "        DataLists.append(result['Confirm'])\n",
    "        experiment_counter = result['experiment_counter'] \n",
    "\n",
    "print('done step 6.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert float64 data to float32 to reduce memory load (can also convert to 16 if needed)\n",
    "\n",
    "For plain english:\n",
    "\n",
    "float16 = 4 decimal points\n",
    "\n",
    "float32 = 8 decimal points\n",
    "\n",
    "float64 = 16 decimal points\n",
    "\n",
    "more decimal points = more data/memory that computer has to keep track of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out this section in case memory load needs to be reduced\n",
    "\n",
    "for n in tqdm(DataLists[1:]):\n",
    "    print(n)\n",
    "    TestData=n\n",
    "    TestData[TestData.select_dtypes(np.float64).columns]=TestData.select_dtypes(np.float64).astype(np.float16)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Grouping Data and Naming\n",
    "\n",
    "This step takes all the individual strain data (processed in Step 6) and combines them into single dataframe, filters for time window 490s - 590s, drops unwanted columns. \n",
    "\n",
    "The final processed data `Baseline_data` is ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline_data=pd.concat(df.assign(dataset=StrainNames.get(i+1)) for i, df in enumerate(DataLists[1:]))\n",
    "\n",
    "Baseline_data[['Gene', 'Allele']] = Baseline_data['dataset'].str.split(pat='_', n=1, expand=True)\n",
    "\n",
    "Baseline_data['Allele'] = Baseline_data['Allele'].fillna('N2')\n",
    "\n",
    "Baseline_data = Baseline_data.drop(columns=[\"Tap\", \"Puff\", \"x\",\"y\", \"Experiment\"]).dropna().reset_index(drop=True)\n",
    "\n",
    "Baseline_data['Screen']=Screen\n",
    "\n",
    "Baseline_data = Baseline_data[((Baseline_data.Time<=590)&(Baseline_data.Time >=490))] \n",
    "\n",
    "Baseline_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Post Stimulus Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar filters as baseline data\n",
    "\n",
    "Post_stimulus_data_pre=pd.concat(df.assign(dataset=StrainNames.get(i+1)) for i,df in enumerate(DataLists[1:]))\n",
    "\n",
    "Post_stimulus_data_pre[['Gene', 'Allele']] = Post_stimulus_data_pre['dataset'].str.split('_', n=1, expand=True)\n",
    "\n",
    "Post_stimulus_data_pre['Allele'] = Post_stimulus_data_pre['Allele'].fillna('N2')\n",
    "\n",
    "Post_stimulus_data_pre = Post_stimulus_data_pre.drop(columns=[\"Puff\", \"x\",\"y\"]).dropna().reset_index(drop=True)\n",
    "\n",
    "Post_stimulus_data_pre['Screen']=Screen\n",
    "\n",
    "Post_stimulus_data_pre = Post_stimulus_data_pre[((Post_stimulus_data_pre.Time>599))]\n",
    "\n",
    "Post_stimulus_data_pre['Time'] = round(Post_stimulus_data_pre['Time']).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add continuous tap numbers from 1 to 31 for each experiment\n",
    "# E.g., Experiment 1 has taps 1-31, Experiment 2 has taps 1-31 and so on..\n",
    "\n",
    "Post_stimulus_data_pre['Tap_num'] = Post_stimulus_data_pre.groupby(['Experiment'])['Tap'].cumsum()\n",
    "\n",
    "Post_stimulus_data_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windows from 7s to 9.5s post a tap (\"Tap\"=1) for each experiment\n",
    "# and concatenate all these wondows into a single dataframe\n",
    "\n",
    "Post_stimulus_data = []\n",
    "\n",
    "for exp in Post_stimulus_data_pre['Experiment'].unique(): # loop through each experiment separately \n",
    "    df = Post_stimulus_data_pre[Post_stimulus_data_pre['Experiment'] == exp]  \n",
    "    tap_times = df[df['Tap'] == 1]['Time']  # get times where tap occured\n",
    "\n",
    "    for t in tap_times: \n",
    "        window = df[(df['Time'] >= t + 7) & (df['Time'] <= t + 9.5)]\n",
    "        Post_stimulus_data.append(window)\n",
    "\n",
    "Post_stimulus_data = pd.concat(Post_stimulus_data)\n",
    "\n",
    "Post_stimulus_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate columns by \"Experiment\" + \"Tap_num\" by taking their means\n",
    "\n",
    "Post_stimulus_data = Post_stimulus_data.groupby(['Experiment', 'Tap_num','Screen','Date','Plate_id','Gene','Allele','dataset']).agg({\n",
    "    'Time': 'min', # take minimum valu of time instead of mean\n",
    "    'n': 'mean',\n",
    "    'Number': 'mean',\n",
    "    'Instantaneous Speed': 'mean',\n",
    "    'Interval Speed' : 'mean',\n",
    "    'Bias': 'mean',\n",
    "    'Tap': 'mean',\n",
    "    'Morphwidth': 'mean',\n",
    "    'Midline': 'mean',\n",
    "    'Area': 'mean',\n",
    "    'Angular Speed': 'mean',\n",
    "    'Aspect Ratio': 'mean',\n",
    "    'Kink': 'mean',\n",
    "    'Curve': 'mean',\n",
    "    'Crab': 'mean',\n",
    "    'Pathlength': 'mean'\n",
    "})\n",
    "\n",
    "Post_stimulus_data = Post_stimulus_data.reset_index()\n",
    "\n",
    "Post_stimulus_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done step 7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframe as `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline_data.to_csv(f\"{Screen}_baseline_output.csv\")\n",
    "print('saved Baseline data as .csv!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_stimulus_data.to_csv(f\"{Screen}_post_stimulus.csv\")\n",
    "print('saved Post stimulus data as .csv!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
